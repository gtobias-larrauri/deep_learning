\documentclass{article}
\usepackage[a4paper,margin=1in,footskip=0.25in]{geometry}

\usepackage{graphicx} % Required for inserting images
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}


\newcommand{\tblue}[1]{\textcolor{blue}{#1}}
\newcommand{\tgreen}[1]{\textcolor{green}{#1}}
\newcommand{\tred}[1]{\textcolor{red}{#1}}


\graphicspath{.}
\title{Deep learning notes}
\author{Guillem Tob√≠as Larrauri}
\date{January 2025}

\begin{document}

\maketitle


\section{Theory: Deep learning goodfellow 2016}
This first section is structured into math basics, then modern practical deep networks
and finally some research topics(which are now maybe also working in practice)
\subsection{Chap 5: ML basics}
Many tasks you can do in Machine learning, they are cooler that just regression and classification.

\begin{itemize}
    \item classification
    \item classification with missing inputs, biomed sciences!
    \item regression
    \item transcription and language related stuff: from image to text
    \item structrued output, output is a vector
    \item anomaly detection
    \item synthesis and sampling
    \item imputation of missing values
    \item denoising
    \item density estimation: read math book.
\end{itemize}


\subsection{Chapter 6: Deep Feedforward Networks}

No info going backwards in the layers of the model.
The question is then how to choose the mapping $\psi$ .

The strategy of deep learning is to learn $\psi$ which is often in a parametrized form,
this way there need not be manual feature engineering, just find the right class of general functions. It is truly like regular statistics.

The XOR problem shows how a linear classifier would fail and so we need to find some transformation of the underlying space that allows for better classification.
If we think we are choosing amongst a large class of functions we can view the loss as a functional, mapping from a set of functions to a real number. And we find the best function.

Finding optimums of functionals is from calculus of variations, which is complicated.
However, we have the typical result of the \textit{conditional expectation} being optimal with squared error loss. Suposedly mse and mae are pretty bad for classification because some saturation stuff.

\subsubsection{Output functions}
\textbf{LENGTHY DISCUSSION on sigmoid... recheck}
\tblue{Gaussian mixture outputs}
\subsubsection{Hidden units}
\begin{itemize}
    \item relu 
    \item sigmoid
    \item maxout, check in more depth
\end{itemize}
Unless indicated otherwise, most hidden units can be described as accepting
a vector of inputs $ \textbf{x} $, computing an affine transformation $\textbf{z} = \textbf{W}'\textbf{x} + \textbf{b}$
and then applying an element wise nonlinear function $g(\textbf{z})$ most hidden units only 
different is the choice of this activation function g(*)

\subsubsection{Architecture design}
Some theorems but mainly trial and error.

\subsubsection{Backpropagation to compute gradients}
\tblue{Check out my math book it has very good example}

\subsection{Regularization for deep learning}
A first step is to add a ridge penalty to the overall loss function.


\subsubsection{Dataset augmentation}
Create fake data, in terms of images you can try to rescale it or rotate it.


\section{Math: From Mathematics for Machine learning}
\tblue{This will be copied over and is needed for ML and also Deep and Graph learning,
I will be adding notes on which chapters matter most for where.}

\subsection{Linear Algebra}
An algebra is a set of objects and a set of rules to manipulate these objects.
Vectors are objects such that they can be added together and multiplied by a constant and the result is still a vector.
\begin{center}
    \includegraphics[scale=0.8]{2linalg.png} % Adjust scale as needed
\end{center}
\subsubsection{Systems of linear equations}
We will mostly ignore for now.

\subsubsection{Matrices}
A matrix as an m * n tuple of elements $a_{ij}$, we define it as \textbf{A}
We have some important properties of all matrices \tblue{TO DO!}
$\forall \textbf{A} \in \mathbb{R}^{m \times n} : \textbf{I}_m \textbf{A} = \textbf{A} \textbf{I}_n = \textbf{A} $

The transpose and the inverse (which not always exists, but when it exists it is unique.)
Some important properties of transpose and inverse:
\begin{itemize}
    \item $\textbf{A} \textbf{A}^{-1} = \textbf{I} = \textbf{A}^{-1}\textbf{A}$
    \item $(\textbf{A}\textbf{B})^{-1} = \textbf{B}^{-1} \textbf{A}^{-1}$
    \item $(\textbf{A} + \textbf{B})^{-1} \neq \textbf{A}^{-1} + \textbf{B}^{-1}$
    \item $(\textbf{A}^T)^{T} = \textbf{A}$
    \item $(\textbf{A} + \textbf{B})^T = \textbf{A}^T + \textbf{B}^T$
    \item $\textbf{AB}^T = \textbf{B}^T \textbf{A}^T$
\end{itemize}
A matrix is symmetric if it is equal to its transpose and we also have $(\textbf{A}^{-1})^T = (\textbf{A}^T)^{-1} =: \textbf{A}^{-T}$
For multiplication by a scalar all properties are super intuitive.

\subsubsection{Solving systems of linear equations, we skip for now}
We can represent compactly via $\textbf{A} \textbf{x} = \textbf{b}$

If the matrix \textbf{A} is square we can directly compute the solution (if it exists) as $\textbf{x} = \textbf{A}^{-1} \textbf{b}$
If the columns of a non square matrix are linearly independent we can compute the following:
$\textbf{A} \textbf{x} = \textbf{b} \iff \textbf{A}^T \textbf{A} \textbf{x} = \textbf{A}^T \textbf{b} \iff \textbf{x} =
 (\textbf{A}^T \textbf{A})^{-1}\textbf{A}^T \textbf{b}    
$

\subsubsection{Vector spaces}
\tblue{Add the math if needed.}
\textit{Groups:} 
We consider a set $\mathcal{G}$ and an operation $ \otimes : \mathcal{G} \times \mathcal{G} \longrightarrow \mathcal{G}  $
\begin{itemize}
    \item Closure: For any x and y in the group, if we apply the operation to x and y the result will still be in the group
    \item Associativity For any 3 in the group the parentheses can be changed meaning we do one operation first or second and we get the same result.
    \item Neutral element: There exists in the group a neutral element such that if we apply the operation to anything in the group the result will be the original thing
    \item Inverse element: There exists for every element in the group another element such that if we apply the operation we will get the neutral element.
\end{itemize}
If we have commutative property (x * y = y * x) we have \textbf{Abelian group.}
We then have some examples of what is and what is not a group.
\\
\\
\textbf{Vector spaces}
A vector space will now have two operations, an inner operation and an outer operation.
\begin{itemize}
    \item  $ + : \mathcal{V} \times \mathcal{V} \longrightarrow \mathcal{V} $
    \item $* : \mathbb{R} \times \mathcal{V} \longrightarrow \mathcal{V} $
\end{itemize}
And:
\begin{itemize}
    \item $(\mathcal{V}, + )$ is an Abelian group.
    \item We have distributivity with respect to the scalar/outer operation 
    \item We have Associativity with respect to outer operation
    \item There exists a neutral element with respect to outer operation
\end{itemize}
Our inner operation will therefore be vector addition and the outer will be multiplication by scalars
Vector multiplication is not defined for all as the dimensions may not be the same.

\textbf{Examples:} I don't understand why $\mathbb{R}^{m \times n} $ is a vector space if the inverse may not exist. AH, inverse with respect to inner operation,
meaning find another matrix such that we can get the inverse. And that is always defined by Abelian, meaning commutative under the sum.
\\
\\
\textbf{Vector subspaces:} 
Subsets of the original space such that when we perform the operations (sum and multiplication) we will never
leave the subspace. 

We will naturally inherit many of the properties from the larger vector space.
However we need to show that U is not empty, that the 0 element is in there and that we
have closure with respect to both the outer and inner operations.

\textit{Every subspace of Rn isthe solution space of a system of homogeneous linear equations.}
I don't fully understand the implications of this.
\\
\\
\subsubsection{Linear independence}
Consider a vector space V and a finite number of vectors (k). Remember matrices are also vectors in the R nm vector space.

Every $\textbf{v} \in V $ of the form $\textbf{v} = \sum_{i=1}^{k}\lambda_i \textbf{x}_i \in V$
where the lambdas are real numbers are a linear combination of the initial vectors x.
Notice we apply both operations, the inner operation in the sum and the outer operation.
$\textbf{0}$ can always be written as a linear combination.
\\
\\
The notion of \textbf{linear independence} is the following: If from the original vectors we
can build a non trivial linear combination that gives us the $\textbf{0}$ vector the original vectors are linearly dependent.
If only setting all lambdas to 0 gives us the solution the vectors are linearly independent.
\textit{There are a set of practical rules in the book such that we show if they are independent or not, you can also solve the whole system (pain)}
\\
\\
\tblue{The intuition is that if we drop any of the original vectors we are losing informaiton!}
tblue{Review page 32 prett cool!}
\subsubsection{Basis and rank}
In a vector space we are interested in a subset of it such that any vector in the larger space 
can be generated as a linear combination of the vectors in the smaller set.
The set of all linear combinations is called the span of a set. If a subset A spans the whole vector space V we say 
V = span(A). Generating sets are sets are sets that span the vector space.

A \textbf{Basis} is: The minimal generating set.
Can also be defined in terms of linear independence, it is the maximal linearly independent set of vectors in V
Also uniqueness things, yesyes.
\begin{itemize}
    \item Every vector space has a basis, however there is not unique basis. All basis have the same number of vectors.
    \item To find a basis we can see the original vectors, then solve $\sum_{i=1}^{k}\lambda_i \textbf{x}_i = \textbf{0}$ which can be put in matrix form
    Then find pivot columns and those will be the span. 
    \item Example 2.14 is beautiful \tblue{Solving linear systems is useful!!}
\end{itemize}

The \textbf{Rank} of a matrix is closely defined:
With it we can see the span of the columns and the rows. Columns are m dimensional and so on.
\tblue{Important remarks for solvable system and existance of an inverse, gp back adn write down!}

\subsubsection{Linear mappings}
\tblue{I get lost with this!!! Do a review in the future for now move on.}
Non linear mapping is if the two properties hold individually but not necessarily together.
A linear mapping $\textbf{v}$ from V to W vector if for any two vectors in the original space
and any two real numbers we get $\Phi (\lambda \textbf{x} + \psi  \textbf{y}) =\lambda \Phi ( \textbf{x}) +\psi  \Phi (\textbf{y})$
Therefore the result is still a vector space, added and multiplied is still preserved. It is going to be a different vector space though!
We can represent linear mappings as \textbf{matrices} and we will see more about this.

Special linear mappings:
\begin{itemize}
    \item Injective, if we have two vectors equal after transform they were equal in the original space.
    \item Surjective, we can reach any point in the second vector space from the first vector space using the mapping.
    \item Bijective if 1 and 2 hold
\end{itemize}
Bijective mappings can be undone with the inverse mapping, so this guarantees existance or whatever.
Isomorphic is if linear and bijective from V to W. 

We have a theorem that says that V and W are isomophic iff they have the same dimension.
This justifies treating $\mathbb{R}^{m \times n} = \mathbb{R}^{mn} $ we can unstack the matrix and 
not lose any information. Because there exists a bijective linear mapping that goes from one to the other.

\tblue{There are some extra properties that I can write down in p 38.}
\\
\\
\textbf{Matrix representation of linear mappings:}
Bottom line is that linear mappings can be represented as matrices.


Any n dimensional vector space is isomophic to $\mathbb{R}^n$ 
We will then define B = $(\textbf{b}_1,...,\textbf{b}_n)$ as an ordered basis.

Then for any vector space V with its ordered basis B, we can define any vector inside the space
as $\sum_{i=1}^{n}\alpha_i \textbf{b}_i$ and this representation is unique, and the alphas are 
the coordinate representation of the vector x with basis B.
Let's say we are in R2 and we have the canonical basis ((0,1),(1,0)) to represent (2,2) we will pick alphas to be 2 and 2.
But if we have a different basis (something that equivalently defines the space in R2) our alphas will be very dfifferent. Say our basis 
is now (-1,1) and (0,1), to represent (2,2) we would need aplhas to be (4,2)

\textbf{2.19}
We now have 2 vector spaces, V, W with the following bases B = $(\textbf{b}_1,...,\textbf{b}_n) and C (\textbf{c}_1,...,\textbf{c}_m)$
and we consider a linear mapping from V to W: $\Phi$
Then for all j: $ \Phi(\textbf{b}_j) = \sum_{i=1}^{m} \alpha_{ij}\textbf{c}_i$ 
is the unique representation of $\Phi(\textbf{b}_j)$ with respect to c.

By the definitons we get to $\hat{\textbf{y}} = A_{\Phi}\hat{\textbf{x}}$ where x are coordinates of a vector in the original basis 
and we use the matrix to transform to the new basis 
\textbf{I don't understand example 2.21!!!} Use chatgpt, vector in old space maps to the new one. But the transf matrix is about
basis, each basis element in old space can be represented a certain way in the new one. And because all vectors in a space can
be represented as a linear combination of the basis we can apply this to all vectors in the space.
\textbf{Example 2.22} We use a linear transform to get new coordinates and expand the square applying the transformation matrix.
\\
\\
\textbf{Basis change:}
What happens to transformation matrix if we change the basis in the old or new space 
we can exploit this to find easy transformation matrices. \tblue{For now I skip it.}
\\
\\
\textbf{Image and kernel:}
For linear mappings:

The kernel is the set of vectors in the original space such that applying the linear mapping gives you the 0 vector in the new space.
The image is the set of vectors in the new space by applying the linear mapping into any vector of the old space.
\tblue{Contain many interesting properties but they seem very inutitive, left for another time.}

\subsubsection{Affine spaces}
\tblue{Skipped this for now}


\subsubsection{Exercises and complete above}

\textbf{2,17,...}

\subsection{Analytic geometry}
Goes into geometric interpretation for linear algebra, lengths and distances between two vectors
inner products and norms and finally orthogonal projections.

\begin{center}
    \includegraphics[scale=0.8]{3geom.png} % Adjust scale as needed
\end{center}

\subsubsection{Norms}
A norm is a defined on a vector space and goes from the vector space to a real number. It defines the length of a vector.
To be a proper norm the following need to hold for any two vectors in the space and any real number lambda:
\begin{itemize}
    \item $\| \lambda \textbf{x} \| = abs(\lambda)\|\textbf{x} \|  $
    \item $\|\textbf{x} + \textbf{y} \| \leq \|\textbf{x} \| + \| \textbf{y} \|  $
    \item Positive definite meaning larger or equal than 0 and 0 for the 0 vector.
\end{itemize}
We usually use the Euclidean norm $\sqrt{\textbf{x}^T\textbf{x}}$ which computes
the eucldiean distance from the origin.

\subsubsection{Inner products}
Allow introduction of geometrical concepts like length angle and distance between two vectors.
The simplest is the dot product.

The general definition of a inner product is as a positive definite (no neg distances), symmetric(distance a to b is same as b to a)
and bilinear (why not) such that it maps from $V \times V \rightarrow \mathbb{R} $

Bilinear property: For any x y z in vector space, and two real numbers:
The inner product will satisfy the following: $$
\Omega (\lambda x + \psi y, z) = \lambda\Omega(x,z) + \psi\Omega(y,z)
\quad \Omega (x ,\lambda y + \psi z) = \textrm{the equivalent}
$$
\\
\\
\textbf{Symmetric positive definite matrices}
The inner product is determined thorugh a positive definite matrix (all these matrices are symmetric, so symmetric positive definite)
Because all vectors in the space can be represented through a linear combination of basis functions
we can end up representing the inner product as $$\langle x, y \rangle = \textrm{properties fo the norm, bilinear} =
\sum_{i=1}^{n}\sum_{j=1}^{n} \psi_i \langle b_i, b_j \rangle \lambda_j = \hat{x}'A\hat{y} 
$$
Where the elements of A are norms amongst basis vectors..0



\subsubsection{Lengths and distances}
Any inner product induces a norm: $\| x \Vert := \sqrt( \langle x,x \rangle )$
This induced norm will always satisfy the Cauchy-Schwartz inequality: $| \langle x,y \rangle  |
\leq \| x \Vert \| y \Vert 
$

We can define distances with these elements:

$$d(x,y) := || x- y || = \sqrt{\langle x-y, x-y \rangle} $$

the second step is only if the norm is induced by the inner product which may not always be true.

\textbf{Inner product and distance behave in opposite directions, if large similarity we have large inner product, and low distance, and viceversa.}




\subsubsection{Angles and orthogonality}
By Cauchy Schwartz inequality we can define an angle between two vectors, which can be ignored without too much issue.

\textit{orthogonality:} When inner product between two vectors is 0, if the vectors have norm 1 we call and are orthogonal 
we call them orthonormal.

Matrices are orthogonal if all columns are orthonormal, we define the inner product as dot product here I believe.
$A A' = I = A' A$ the inverse is the transpose. These matrices preserve both angles and distances.

\subsubsection{Orthonormal basis}
Each basis vector is orthogonal to all the others and it has length one.
They can be built via the Gram Schmidt process and they are used for PCA.



\subsubsection{Orthogonal complement}
We have a V (D dimensional) and a subset U (m dimensional), the orthogonal component has dimensions (D-M)
and contains all vectors in V that are orthogonal to every vector in U. 
Morover we can represent any vector in V through a linear combination of the basis of U and the basis of its orthogonal complement.
\tblue{The example is pretty nice}


\subsubsection{Inner prodcuts of functions}
THey are integrals


\subsubsection{Orthogonal projections}
Let V be a vector space and $u \subseteq V$. A linear mapping $\pi: V \rightarrow U$ is called a projection
if $\pi ^2 = \pi \circ \pi = \pi $ Because a linear mapping can be represented via a transformation matrix,
we will call projection matrices those that have this given property.
\\
\\
\textbf{Projection onto one dimensional subspaces}
We are really trying to solve a minimization problem.
\tblue{Read and mostly understood, write down another time. proof of orthogonality by contradiction in notebook}

\tblue{Do it with ols}

\subsection{Matrix decomposition}
Determinants and eigenvalues, Cholesky and Diagonalization and SVD

\subsubsection{Determinant and trace}
The determinant is only defined for square matrices. It determines invertibility, that is if the
determinant is 0 the matrix is not invertible.

The determinant has some properties. Not that used anymore but important for eigen stuff.

The trace is the sum of diagonal elements.

\tblue{Add properties if they come up again in the book.}

\subsubsection{Eigenvalues and eigenvectors:}

Defined from the equation $ \textbf{A}\textbf{x} = \lambda \textbf{x} $
where lambda is the igenvalue for x the eigenvector. The 0 vector is always a solution but we ignore it.

Eigenvectors are not unique they can always be scaled and the result will not change.
Eigenvectors are roots of the characteristic polynomial, that is why $det(\textbf{A} - \lambda \textbf{I}_n) = 0$

All eigenvectors of A span a subspace called the eigenspace.

Some properties:
\begin{itemize}
    \item A matrix and its transpose have the same eigenvalues but not necessarily the same eigenvectors.
    \item The eigenspace is the null space of $\textbf{A} - \lambda \textbf{I}$ 
    \item Something about similar matrices \tblue{Review}
    \item Symmetric positive definite matrices have all positive real eigenvalues.
\end{itemize}
N eigenvectors from a Matrix with distinct eigenvalues are linearly independent, that means they form a basis of Rn.
\\
\\
We can obtain a symmetric positive definite matrix from any $\textbf{A} \in \mathbb{R}^{m \times n} $
by defining the following: $S := \textbf{A}'\textbf{A}$
\\
\\
If A is symmetric there exists an orthonormal basis consisting of eigenvectors of A and all eigenvalues are real.
We can thus decompose a matrix A = PDP' where the P contain the eigenvecrtors are columns.
\\
\\
finally, the determinant of a matrix is the product of eigenvalues and the trace is the sum of eigenvalues.

\subsubsection{Cholesky decomposition}

A symmetric positive definite matrix can be decomposed into a lower triangular matrix and its transpose.
$\textbf{A} = \textbf{L}\textbf{L}'$ 
This lower triangular matrix is unique.
Used in practice

\subsubsection{Eigendecomposition:}
With diagonal matrices and blablabla diagonal is eigenvalues.

\subsubsection{Singular value decomposition}
$$A = U \Sigma V'$$
Where A is m * n, U is m*m, sigma is m * n and V' is n n 
Both U and V are orthonormal and sigma is a diagonal matrix.

\tblue{Cover in much more depth as well as matrix apporximation and the movie example!!!}

\section{Tensorflow}




\end{document}