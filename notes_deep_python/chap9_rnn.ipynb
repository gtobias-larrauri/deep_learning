{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c002f3-0477-425d-8067-651b47269cd3",
   "metadata": {},
   "source": [
    "# Chapter 9: RNN \n",
    "\n",
    "*This is engineering course then let's do it properly on Python.*\n",
    "\n",
    "RNN can be applied to any sequential data but they are mainly used for large language models. Also mention sequences of images and others.\n",
    "\n",
    "Today Transformers (*Chap 11*) have taken some market share from RNN's but RNN's are still useful as an introduction to sequence modelling.\n",
    "\n",
    "\n",
    "\n",
    "**Varying length sequences of fixed shape objects**\n",
    "\n",
    "**To do:**\n",
    "- Check out density modelling with mixture of normals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a271f69-b1cb-4c5a-990c-a45c30d89d8c",
   "metadata": {},
   "source": [
    "## Working with sequences\n",
    "\n",
    "Some datasets consist of a single massive sequence. Consider, for example, the extremely long streams of sensor readings that might be available to climate scientists. In such cases, we might create training datasets by randomly sampling subsequences of some predetermined length. More often, our data arrives as a collection of sequences. Consider the following examples: (i) a collection of documents, each represented as its own sequence of words, and each having its own length ; (ii) sequence representation of patient stays in the hospital, where each stay consists of a number of events and the sequence length depends roughly on the length of the stay.\n",
    "\n",
    "*If we have a collection of sequences we can assume i.i.d over sequences but not within.* For most sequence models, we do not require independence, or even stationarity, of our sequences. Instead, we require only that the sequences themselves are sampled from some fixed underlying distribution over entire sequences.\n",
    "\n",
    "\n",
    "AR Models: $p(x_t|x_{t-1},x_{t-2},x_{t-3},...,)$\n",
    "\n",
    "Latent AR Models very nice picture for intuition in the book, h is unobserved: $$\\hat{x_t} = P(x_t|h_t) \\quad h_t = g(h_{t-1},x_{t-1}) $$\n",
    "\n",
    "To construct training data from historical data, one typically creates examples by sampling windows randomly. We use stationarity here.\n",
    "\n",
    "We can model the joint probability of words by using conditional probability definition and thus making **language models**: \n",
    "\n",
    "$$P(x_1,...,x_T) = P(x_1)\\prod_{t=2}^{T} P(x_t|x_{t-1},...,x_1)$$\n",
    "\n",
    "**Markov assumption:** Conditioning on a few past is enough, today's large models incorporate thousands of past observations. \n",
    "\n",
    "**Updating:** How to update model for an extra period/word: $$P(x_{t+1},...,x_1) = P(x_t,...,x_1)P(x_{t+1}|x_t,...,x_1)$$ First part of RHS is the model we had and the other part is density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b97737-2d46-4eeb-9d9a-baee5788869e",
   "metadata": {},
   "source": [
    "## From text to sequence data: What we did in Week 7 seminar.\n",
    "\n",
    "I have a feeling the second task from DL will require this so no need to worry for now.\n",
    "\n",
    "tflow: Weird that models takes a sequence of numbers and understand to treat them as categorical directly but whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd5e69-fb7c-49fb-934d-6dd217a521fe",
   "metadata": {},
   "source": [
    "## Perplexity:\n",
    "\n",
    "Evaluation metric for language models, I am perplexed indeed.\n",
    "ok not so bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5225b-a392-4a73-b59d-baea2b0b0509",
   "metadata": {},
   "source": [
    "## RNN:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
